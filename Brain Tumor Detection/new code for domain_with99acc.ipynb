{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kXVSo4x1gSeSug-CxEdZlo4qAnZOIQQ4","timestamp":1758390050967}],"authorship_tag":"ABX9TyPN464gLsgx3qUXeWSDfxRu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3RkSy3BCAC5","executionInfo":{"status":"ok","timestamp":1759244462915,"user_tz":-330,"elapsed":2382,"user":{"displayName":"Amit","userId":"12478379899321160573"}},"outputId":"49dda710-f084-405d-9d98-8090cce4ad1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pk5dUrrY8SJn"},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","import timm\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from tqdm import tqdm\n","import os\n","import copy\n","import matplotlib.pyplot as plt\n","import numpy as np\n"]},{"cell_type":"code","source":["import os\n","import shutil\n","\n","def remove_unwanted_folders(root, unwanted_folders=[\"Br35H-Mask-RCNN\"]):\n","    for uf in unwanted_folders:\n","        for subdir, dirs, files in os.walk(root):\n","            path_to_remove = os.path.join(subdir, uf)\n","            if os.path.exists(path_to_remove):\n","                shutil.rmtree(path_to_remove)\n","                print(f\"Removed {path_to_remove}\")\n","\n","# Apply to BR35H\n","br_root = \"/content/drive/MyDrive/Data/Br35H\"\n","remove_unwanted_folders(br_root)\n"],"metadata":{"id":"-ekx65anFH7G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for root, dirs, files in os.walk(br_root):\n","    print(root, dirs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zj2ag5WkFLyj","executionInfo":{"status":"ok","timestamp":1759244479587,"user_tz":-330,"elapsed":26,"user":{"displayName":"Amit","userId":"12478379899321160573"}},"outputId":"fb759237-2527-432e-d15c-4ed40626ccaf","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Data/Br35H ['yes', 'no', 'pred', 'train', 'test']\n","/content/drive/MyDrive/Data/Br35H/yes []\n","/content/drive/MyDrive/Data/Br35H/no []\n","/content/drive/MyDrive/Data/Br35H/pred []\n","/content/drive/MyDrive/Data/Br35H/train ['tumor', 'no_tumor']\n","/content/drive/MyDrive/Data/Br35H/train/tumor []\n","/content/drive/MyDrive/Data/Br35H/train/no_tumor []\n","/content/drive/MyDrive/Data/Br35H/test ['tumor', 'no_tumor']\n","/content/drive/MyDrive/Data/Br35H/test/tumor []\n","/content/drive/MyDrive/Data/Br35H/test/no_tumor []\n"]}]},{"cell_type":"code","source":["\n","\n","\n","import os\n","\n","br_root = \"/content/drive/MyDrive/Data/Br35H\"\n","\n","for split in ['train', 'test']:\n","    for cls in ['tumor', 'no_tumor']:\n","        os.makedirs(os.path.join(br_root, split, cls), exist_ok=True)\n","\n","print(\"Folder structure created!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6EZmVmX4FVyH","executionInfo":{"status":"ok","timestamp":1759244480712,"user_tz":-330,"elapsed":23,"user":{"displayName":"Amit","userId":"12478379899321160573"}},"outputId":"9aecded0-bdeb-4bd7-f2ea-eee556a56b64","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Folder structure created!\n"]}]},{"cell_type":"code","source":["import glob\n","import shutil\n","import random\n","\n","# Example for yes -> tumor\n","yes_images = glob.glob(os.path.join(br_root, \"yes\", \"*.*\"))\n","random.shuffle(yes_images)\n","split_idx = int(0.8 * len(yes_images))\n","\n","for img in yes_images[:split_idx]:\n","    shutil.move(img, os.path.join(br_root, \"train\", \"tumor\"))\n","for img in yes_images[split_idx:]:\n","    shutil.move(img, os.path.join(br_root, \"test\", \"tumor\"))\n","\n","# Example for no -> no_tumor\n","no_images = glob.glob(os.path.join(br_root, \"no\", \"*.*\"))\n","random.shuffle(no_images)\n","split_idx = int(0.8 * len(no_images))\n","\n","for img in no_images[:split_idx]:\n","    shutil.move(img, os.path.join(br_root, \"train\", \"no_tumor\"))\n","for img in no_images[split_idx:]:\n","    shutil.move(img, os.path.join(br_root, \"test\", \"no_tumor\"))\n"],"metadata":{"id":"9kZr6094FVux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for split in ['train', 'test']:\n","    for cls in ['tumor', 'no_tumor']:\n","        path = os.path.join(br_root, split, cls)\n","        print(path, \":\", os.listdir(path)[:5])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Apg4ZdZiFLu3","executionInfo":{"status":"ok","timestamp":1759244482326,"user_tz":-330,"elapsed":14,"user":{"displayName":"Amit","userId":"12478379899321160573"}},"outputId":"479f537d-ed52-494c-b78c-c61fcbaf7931","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Data/Br35H/train/tumor : ['y1214.jpg', 'y1215.jpg', 'y1197.jpg', 'y1190.jpg', 'y1211.jpg']\n","/content/drive/MyDrive/Data/Br35H/train/no_tumor : ['no1227.jpg', 'no1243.jpg', 'no1309.jpg', 'no1255.jpg', 'no1284.jpg']\n","/content/drive/MyDrive/Data/Br35H/test/tumor : ['y1035.jpg', 'y1106.jpg', 'y1052.jpg', 'y1022.jpg', 'y1040.jpg']\n","/content/drive/MyDrive/Data/Br35H/test/no_tumor : ['no1034.jpg', 'No17.jpg', 'no1013.jpg', 'no1002.jpg', 'no1001.jpg']\n"]}]},{"cell_type":"code","source":["from torchvision import transforms\n","\n","img_size = 224\n","\n","# Train on BR35H with strong augmentations\n","train_tfms = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.RandomAffine(0, translate=(0.1,0.1)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","\n","test_tfms = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","\n","\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","\n","\n","br_root = \"/content/drive/MyDrive/Data/Br35H\"\n","btd_root = \"/content/drive/MyDrive/Data/Brain Tumor Data\"\n","\n","# Datasets\n","br_train_ds = ImageFolder(f\"{br_root}/train\", transform=train_tfms)\n","btd_test_ds = ImageFolder(f\"{btd_root}/test\", transform=test_tfms)\n","\n","# DataLoaders\n","train_loader = DataLoader(br_train_ds, batch_size=32, shuffle=True, num_workers=2)\n","test_loader  = DataLoader(btd_test_ds, batch_size=32, shuffle=False, num_workers=2)\n","\n","num_classes = len(br_train_ds.classes)\n","print(\"Classes:\", br_train_ds.classes)\n","print(\"Training images:\", len(br_train_ds))\n","print(\"BTD test images:\", len(btd_test_ds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKFRJC3u8YrH","executionInfo":{"status":"ok","timestamp":1759244483081,"user_tz":-330,"elapsed":38,"user":{"displayName":"Amit","userId":"12478379899321160573"}},"outputId":"5c8395b8-02b9-4f4c-ac3e-0e32506b72ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classes: ['no_tumor', 'tumor']\n","Training images: 2400\n","BTD test images: 1311\n"]}]},{"cell_type":"code","source":["import torch\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    y_true, y_pred = [], []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            preds = torch.argmax(outputs, dim=1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(preds.cpu().numpy())\n","\n","    acc = accuracy_score(y_true, y_pred)\n","    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n","    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n","    f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    return {\n","        \"accuracy\": acc,\n","        \"precision\": prec,\n","        \"recall\": rec,\n","        \"f1\": f1,\n","        \"confusion_matrix\": cm,\n","        \"y_true\": y_true,\n","        \"y_pred\": y_pred\n","    }\n"],"metadata":{"id":"Qxzn6vcl6yyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Subset\n","\n","\n","btd_train_ds = ImageFolder(f\"{btd_root}/train\", transform=train_tfms)\n","few_shot_indices = list(range(int(0.5 * len(btd_train_ds))))\n","btd_few_shot_ds = Subset(btd_train_ds, few_shot_indices)\n","\n","few_shot_loader = DataLoader(btd_few_shot_ds, batch_size=32, shuffle=True, num_workers=2)\n","print(\"Few-shot BTD samples for fine-tuning:\", len(btd_few_shot_ds))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nqq8TTHq6yqy","executionInfo":{"status":"ok","timestamp":1759244484766,"user_tz":-330,"elapsed":92,"user":{"displayName":"Amit","userId":"12478379899321160573"}},"outputId":"8b120e99-2d31-40e0-a670-8675037e5b90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Few-shot BTD samples for fine-tuning: 2856\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","def fine_tune_model(model, few_shot_loader, epochs=10, lr=1e-5):\n","    model.train()\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for images, labels in tqdm(few_shot_loader, desc=f\"Fine-tune Epoch {epoch+1}/{epochs}\"):\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(few_shot_loader):.4f}\")\n","\n","    return model\n"],"metadata":{"id":"TTvnmemA6yiH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class MixStyle(nn.Module):\n","    def __init__(self, p=0.5, alpha=0.1):\n","        super().__init__()\n","        self.p = p\n","        self.alpha = alpha\n","\n","    def forward(self, x):\n","        if not self.training or torch.rand(1) > self.p:\n","            return x\n","        B, C, H, W = x.size()\n","        x_ = x.view(B, C, -1)\n","        mu = x_.mean(dim=2, keepdim=True)\n","        sigma = x_.std(dim=2, keepdim=True)\n","        mu_shuffle = mu[torch.randperm(B)]\n","        sigma_shuffle = sigma[torch.randperm(B)]\n","        x = (x_ - mu) / (sigma + 1e-6) * (sigma_shuffle + 1e-6) + mu_shuffle\n","        return x.view(B, C, H, W)\n"],"metadata":{"id":"nuQJ2--J8YoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def get_model(name, num_classes=4, use_mixstyle=False):\n","    model = None\n","\n","\n","\n","    if name in [\"resnet50\",\"mobilenetv2_100\",\"convnext_tiny\", \"swin_tiny_patch4_window7_224\", \"densenet121\",\"efficientnet_v2_s\"]:\n","        model = timm.create_model(name, pretrained=True, num_classes=num_classes)\n","\n","        if use_mixstyle:\n","\n","            if \"resnet\" in name:\n","                if hasattr(model, \"layer1\"):\n","                    model.layer1[0].bn1 = nn.Sequential(model.layer1[0].bn1, MixStyle(p=0.5, alpha=0.1))\n","\n","            elif \"convnext\" in name:\n","                if hasattr(model, \"stages\") and hasattr(model.stages[0], \"blocks\"):\n","                    first_block = model.stages[0].blocks[0]\n","                    first_block.norm = nn.Sequential(first_block.norm, MixStyle(p=0.5, alpha=0.1))\n","\n","            if \"mobilenetv2\" in name and hasattr(model, \"features\"):\n","                model.features[0][0] = nn.Sequential(model.features[0][0], MixStyle(p=0.5, alpha=0.1))\n","\n","\n","            elif \"densenet\" in name and hasattr(model, \"features\"):\n","                model.features.norm0 = nn.Sequential(model.features.norm0, MixStyle(p=0.5, alpha=0.1))\n","\n","            elif \"swin\" in name and hasattr(model, \"patch_embed\"):\n","                model.patch_embed.proj = nn.Sequential(model.patch_embed.proj, MixStyle(p=0.5, alpha=0.1))\n","\n","\n","    elif name == \"medvit\":\n","        model = timm.create_model(\"vit_small_patch16_224\", pretrained=True, num_classes=num_classes)\n","    else:\n","        raise ValueError(f\"Unknown model: {name}\")\n","\n","    return model\n"],"metadata":{"id":"prXRsfU78YmQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, test_loader, model_name, use_mixstyle=False,\n","                epochs=45, patience=7, fine_tune=False,\n","                lr=1e-5, checkpoint_dir=\"/content/drive/MyDrive/checkpoints\"):\n","\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    model = model.to(device)\n","\n","\n","    if fine_tune:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","        if hasattr(model, \"fc\"):  # ResNet\n","            for param in model.fc.parameters():\n","                param.requires_grad = True\n","        elif hasattr(model, \"classifier\"):  # MobileNet / DenseNet / ConvNeXt\n","            for param in model.classifier.parameters():\n","                param.requires_grad = True\n","        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n","    else:\n","        for param in model.parameters():\n","            param.requires_grad = True\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience)\n","\n","    best_loss = float('inf')\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    trigger_times = 0\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        for images, labels in tqdm(train_loader, desc=f\"{model_name} Epoch {epoch+1}/{epochs}\"):\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n","        scheduler.step(avg_loss)\n","\n","        # Save best checkpoint\n","        if avg_loss < best_loss:\n","            best_loss = avg_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            save_path = f\"{checkpoint_dir}/{model_name}_best.pth\"\n","            torch.save(model.state_dict(), save_path)\n","            print(f\"‚úÖ Saved best checkpoint: {save_path}\")\n","            trigger_times = 0\n","        else:\n","            trigger_times += 1\n","            if trigger_times >= patience:\n","                print(f\"‚èπ Early stopping at epoch {epoch+1}\")\n","                break\n","\n","    model.load_state_dict(best_model_wts)\n","\n","\n","    model.eval()\n","    y_true, y_pred = [], []\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            preds = torch.argmax(outputs, dim=1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(preds.cpu().numpy())\n","\n","    acc = accuracy_score(y_true, y_pred)\n","    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n","    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n","    f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"confusion_matrix\": cm}\n"],"metadata":{"id":"c64achzf803E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def grad_cam(model, image, target_class=None, layer_name=\"blocks.11.norm1\"):\n","    \"\"\"\n","    Simple Grad-CAM hook for transformer-based models\n","    \"\"\"\n","    model.eval()\n","    image = image.unsqueeze(0).to(device)\n","    gradients = {}\n","    activations = {}\n","\n","    def save_gradients(module, grad_input, grad_output):\n","        gradients['value'] = grad_output[0]\n","\n","    def save_activations(module, input, output):\n","        activations['value'] = output\n","\n","    layer = dict([*model.named_modules()])[layer_name]\n","    layer.register_forward_hook(save_activations)\n","    layer.register_backward_hook(save_gradients)\n","\n","    outputs = model(image)\n","    if target_class is None:\n","        target_class = outputs.argmax().item()\n","\n","    model.zero_grad()\n","    outputs[0, target_class].backward()\n","\n","    grad = gradients['value'][0].cpu().detach().numpy()\n","    act = activations['value'][0].cpu().detach().numpy()\n","\n","    weights = grad.mean(axis=-1).mean(axis=-1)\n","    cam = np.zeros(act.shape[-2:])\n","    for i, w in enumerate(weights):\n","        cam += w * act[i]\n","    cam = np.maximum(cam, 0)\n","    cam = cam / cam.max()\n","\n","    plt.imshow(cam, cmap='jet', alpha=0.5)\n","    plt.axis('off')\n","    plt.show()\n"],"metadata":{"id":"22ZHPjH78Yec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","import pandas as pd\n","from torch.utils.data import Subset, DataLoader, Dataset\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Models and MixStyle support\n","models = [\"resnet50\", \"mobilenetv2_100\", \"convnext_tiny\", \"swin_tiny_patch4_window7_224\", \"densenet121\", \"medvit\"]\n","cnn_mixstyle_models = [\"resnet50\", \"mobilenetv2_100\", \"convnext_tiny\", \"densenet121\"]\n","\n","# Directory to save results\n","results_dir = \"/content/drive/MyDrive/Domain_Generalization\"\n","os.makedirs(results_dir, exist_ok=True)\n","checkpoint_dir = \"/content/drive/MyDrive/checkpoints1\"\n","results = {}\n","\n","\n","btd_label_map = {0:1, 1:1, 2:0, 3:1}\n","\n","class BinaryBTDDataset(Dataset):\n","    def __init__(self, subset, mapped_labels):\n","        self.subset = subset\n","        self.labels = mapped_labels\n","    def __len__(self):\n","        return len(self.subset)\n","    def __getitem__(self, idx):\n","        img, _ = self.subset[idx]\n","        return img, self.labels[idx]\n","\n","btd_train_ds = ImageFolder(f\"{btd_root}/train\", transform=train_tfms)\n","few_shot_indices = list(range(int(0.5 * len(btd_train_ds))))  # 10% for few-shot\n","few_shot_labels = [btd_label_map[btd_train_ds.imgs[i][1]] for i in few_shot_indices]\n","btd_few_shot_ds = Subset(btd_train_ds, few_shot_indices)\n","\n","few_shot_loader_binary = DataLoader(\n","    BinaryBTDDataset(btd_few_shot_ds, few_shot_labels),\n","    batch_size=32, shuffle=True, num_workers=2\n",")\n","\n","\n","btd_test_labels = [btd_label_map[label] for _, label in btd_test_ds.imgs]\n","test_loader_binary = DataLoader(\n","    BinaryBTDDataset(btd_test_ds, btd_test_labels),\n","    batch_size=32, shuffle=False, num_workers=2\n",")\n","\n","\n","for m in models:\n","\n","    model = get_model(m, num_classes=2, use_mixstyle=False).to(device)\n","\n","    baseline_ckpt = f\"{checkpoint_dir}/{m}_baseline_best.pth\"\n","    if os.path.exists(baseline_ckpt):\n","        model.load_state_dict(torch.load(baseline_ckpt, map_location=device))\n","\n","    print(f\"\\nüîπ Fine-tuning {m}_baseline on few-shot BTD\")\n","    model = fine_tune_model(model, few_shot_loader_binary, epochs=45, lr=1e-5)\n","\n","    metrics = evaluate_model(model, test_loader_binary)\n","    results[f\"{m}_baseline_finetune\"] = metrics\n","    print(f\"{m}_baseline_finetune: Acc={metrics['accuracy']:.6f}, F1={metrics['f1']:.6f}\")\n","\n","\n","    if m in cnn_mixstyle_models:\n","        model = get_model(m, num_classes=2, use_mixstyle=True).to(device)\n","\n","        mixstyle_ckpt = f\"{checkpoint_dir}/{m}_mixstyle_best.pth\"\n","        if os.path.exists(mixstyle_ckpt):\n","            model.load_state_dict(torch.load(mixstyle_ckpt, map_location=device))\n","\n","        print(f\"\\nüîπ Fine-tuning {m}_mixstyle on few-shot BTD\")\n","        model = fine_tune_model(model, few_shot_loader_binary, epochs=10, lr=1e-5)\n","\n","        metrics_mix = evaluate_model(model, test_loader_binary)\n","        results[f\"{m}_mixstyle_finetune\"] = metrics_mix\n","        print(f\"{m}_mixstyle_finetune: Acc={metrics_mix['accuracy']:.6f}, F1={metrics_mix['f1']:.6f}\")\n","\n","df_results = pd.DataFrame(results).T\n","csv_path = f\"{results_dir}/all_model_evaluation_finetune.csv\"\n","df_results.to_csv(csv_path)\n","print(f\"\\n‚úÖ Evaluation results saved to {csv_path}\")\n","\n","df_results\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"B_hs0ojC8cK2","executionInfo":{"status":"error","timestamp":1759246755564,"user_tz":-330,"elapsed":1664568,"user":{"displayName":"Amit","userId":"12478379899321160573"}},"outputId":"1e47a138-789d-460c-bc5a-fac44a351fba","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîπ Fine-tuning resnet50_baseline on few-shot BTD\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 1/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:45<00:00,  1.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.6513\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 2/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Loss: 0.5169\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 3/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Loss: 0.4087\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 4/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Loss: 0.3240\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 5/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Loss: 0.2689\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 6/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:37<00:00,  2.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, Loss: 0.2343\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 7/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, Loss: 0.2022\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 8/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, Loss: 0.1863\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 9/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, Loss: 0.1667\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 10/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, Loss: 0.1513\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 11/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, Loss: 0.1396\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 12/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, Loss: 0.1324\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 13/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, Loss: 0.1281\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 14/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:42<00:00,  2.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, Loss: 0.1183\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 15/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, Loss: 0.1123\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 16/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, Loss: 0.1100\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 17/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, Loss: 0.1070\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 18/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, Loss: 0.0997\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 19/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, Loss: 0.0961\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 20/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, Loss: 0.0997\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 21/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, Loss: 0.0902\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 22/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:34<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, Loss: 0.0879\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 23/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, Loss: 0.0838\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 24/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, Loss: 0.0815\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 25/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, Loss: 0.0781\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 26/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:34<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, Loss: 0.0700\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 27/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, Loss: 0.0700\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 28/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, Loss: 0.0668\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 29/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29, Loss: 0.0627\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 30/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30, Loss: 0.0612\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 31/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 31, Loss: 0.0555\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 32/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 32, Loss: 0.0554\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 33/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 33, Loss: 0.0518\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 34/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 34, Loss: 0.0546\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 35/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 35, Loss: 0.0466\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 36/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 36, Loss: 0.0474\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 37/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 37, Loss: 0.0435\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 38/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 38, Loss: 0.0402\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 39/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 39, Loss: 0.0394\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 40/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 40, Loss: 0.0373\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 41/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 41, Loss: 0.0372\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 42/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 42, Loss: 0.0318\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 43/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:34<00:00,  2.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 43, Loss: 0.0347\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 44/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:36<00:00,  2.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 44, Loss: 0.0310\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 45/45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:35<00:00,  2.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 45, Loss: 0.0291\n","resnet50_baseline_finetune: Acc=0.929062, F1=0.929832\n","\n","üîπ Fine-tuning resnet50_mixstyle on few-shot BTD\n"]},{"output_type":"stream","name":"stderr","text":["Fine-tune Epoch 1/10:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 54/90 [00:22<00:15,  2.36it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2483669368.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüîπ Fine-tuning {m}_mixstyle on few-shot BTD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfew_shot_loader_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mmetrics_mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3404778733.py\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m(model, few_shot_loader, epochs, lr)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","def summarize_metrics(y_true, y_pred, model_name):\n","    acc = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average=None)  # per-class\n","    recall = recall_score(y_true, y_pred, average=None)\n","    f1 = f1_score(y_true, y_pred, average=None)\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    print(f\"\\n--- {model_name} ---\")\n","    print(\"Accuracy:\", acc)\n","    print(\"Per-class Precision:\", precision)\n","    print(\"Per-class Recall:\", recall)\n","    print(\"Per-class F1:\", f1)\n","    print(\"Confusion Matrix:\\n\", cm)\n","\n","    plt.figure(figsize=(5,4))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","    plt.title(f\"{model_name} Confusion Matrix\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.show()\n","\n","    return {\n","        \"model_name\": model_name,\n","        \"accuracy\": acc,\n","        \"per_class_precision\": precision,\n","        \"per_class_recall\": recall,\n","        \"per_class_f1\": f1,\n","        \"confusion_matrix\": cm\n","    }\n","\n","metrics = train_model(model, train_loader, test_loader, \"resnet50_baseline\")\n","\n","y_true = metrics[\"y_true\"]\n","y_pred = metrics[\"y_pred\"]\n","\n","# Summarize metrics\n","metrics_summary = summarize_metrics(y_true, y_pred, \"resnet50_baseline\")\n"],"metadata":{"id":"tKacoMs0m861"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import ast\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/checkpoints/all_model_evaluation_with_confusion.csv\", index_col=0)\n","\n","labels = [\"No Tumor\", \"Tumor\"]\n","\n","save_dir = \"/content/drive/MyDrive/checkpoints/confusion_matrices\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","for model in df.index:\n","    cm_str = df.loc[model, \"confusion_matrix\"]\n","    cm = np.array(ast.literal_eval(cm_str))\n","\n","    plt.figure(figsize=(4, 3))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n","    plt.title(f\"Confusion Matrix - {model}\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"Actual\")\n","\n","    save_path = os.path.join(save_dir, f\"{model}_confusion_matrix.png\")\n","    plt.savefig(save_path, bbox_inches=\"tight\")\n","    plt.close()\n","\n","print(f\"‚úÖ All confusion matrices saved in: {save_dir}\")\n"],"metadata":{"id":"2jSnsfChgLcc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import ast\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/checkpoints/all_model_evaluation_with_confusion.csv\", index_col=0)\n","\n","\n","labels = [\"No Tumor\", \"Tumor\"]\n","\n","save_dir = \"/content/drive/MyDrive/checkpoints/confusion_matrices\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","pdf_path = os.path.join(save_dir, \"all_confusion_matrices.pdf\")\n","pdf = PdfPages(pdf_path)\n","\n","for model in df.index:\n","    cm_str = df.loc[model, \"confusion_matrix\"]\n","    cm = np.array(ast.literal_eval(cm_str))\n","\n","    plt.figure(figsize=(4, 3))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n","    plt.title(f\"Confusion Matrix - {model}\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"Actual\")\n","\n","    save_path = os.path.join(save_dir, f\"{model}_confusion_matrix.png\")\n","    plt.savefig(save_path, bbox_inches=\"tight\")\n","\n","\n","    pdf.savefig()\n","\n","\n","    plt.show()\n","    plt.close()\n","\n","pdf.close()\n","print(f\"‚úÖ Confusion matrices saved as PNGs in: {save_dir}\")\n","print(f\"‚úÖ Combined PDF saved at: {pdf_path}\")\n"],"metadata":{"id":"-ItsqsZEgTEw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def visualize_grad_cam(model, loader, target_class=None, layer_name=\"blocks.11.norm1\", num_images=3):\n","    model.eval()\n","    images, labels = next(iter(loader))\n","\n","    for i in range(num_images):\n","        image = images[i].unsqueeze(0).to(device)\n","        label = labels[i].item()\n","\n","        gradients = {}\n","        activations = {}\n","\n","\n","        def save_gradients(module, grad_input, grad_output):\n","            gradients['value'] = grad_output[0]\n","\n","        def save_activations(module, input, output):\n","            activations['value'] = output\n","\n","\n","        layer = dict([*model.named_modules()])[layer_name]\n","        layer.register_forward_hook(save_activations)\n","        layer.register_backward_hook(save_gradients)\n","\n","\n","        output = model(image)\n","        if target_class is None:\n","            target_class = output.argmax().item()\n","\n","        model.zero_grad()\n","        output[0, target_class].backward()\n","\n","        grad = gradients['value'][0].cpu().detach().numpy()\n","        act = activations['value'][0].cpu().detach().numpy()\n","\n","        weights = grad.mean(axis=-1).mean(axis=-1)\n","        cam = np.zeros(act.shape[-2:])\n","        for j, w in enumerate(weights):\n","            cam += w * act[j]\n","        cam = np.maximum(cam, 0)\n","        cam = cam / cam.max()\n","\n","\n","        plt.imshow(image[0].permute(1,2,0).cpu().numpy()*0.229 + 0.485)  # denormalize\n","        plt.imshow(cam, cmap='jet', alpha=0.5)\n","        plt.title(f\"Label: {label}, Pred: {target_class}\")\n","        plt.axis('off')\n","        plt.show()\n","\n"],"metadata":{"id":"0INVV95e8YZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","flat_results = {}\n","for model_name, metrics in results.items():\n","    # Keep only scalar metrics (accuracy, f1, precision, recall)\n","    flat_results[model_name] = {k: v for k, v in metrics.items() if k != \"confusion_matrix\"}\n","\n","df_results = pd.DataFrame(flat_results).T\n","df_results.to_csv(\"/content/drive/MyDrive/cross_dataset_ablation_results.csv\")\n","print(\"Saved flattened results to CSV\")\n","df_results\n"],"metadata":{"id":"i_6TmhSpEgUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import matplotlib.pyplot as plt\n","\n","df = pd.read_csv(\"cross_dataset_ablation_results.csv\", index_col=0)\n","\n","cnn_models = [\"resnet50\", \"mobilenetv2_100\", \"convnext_tiny\", \"densenet121\"]\n","\n","metrics = [\"accuracy\", \"f1\"]\n","\n","for metric in metrics:\n","    baseline_vals = [df_results.loc[f\"{m}_baseline\", \"accuracy\"] for m in cnn_models]\n","    mixstyle_vals  = [df_results.loc[f\"{m}_mixstyle\", \"accuracy\"] for m in cnn_models]\n","\n","    x = np.arange(len(cnn_models))\n","    width = 0.35\n","\n","    plt.figure(figsize=(10,5))\n","    plt.bar(x - width/2, baseline_vals, width, label='Baseline')\n","    plt.bar(x + width/2, mixstyle_vals, width, label='MixStyle')\n","    plt.ylabel(metric.capitalize())\n","    plt.xlabel(\"CNN Models\")\n","    plt.title(f\"{metric.capitalize()} Comparison: Baseline vs MixStyle\")\n","    plt.xticks(x, cnn_models)\n","    plt.legend()\n","    plt.show()\n"],"metadata":{"id":"ZmAaP3jQ8YUc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","print(\"y_true counts:\", Counter(y_true))\n","print(\"y_pred counts:\", Counter(y_pred))\n"],"metadata":{"id":"18aetI688YQr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"p0U5WGNq8YOf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1Swjzynw8YMB"},"execution_count":null,"outputs":[]}]}